% !TeX spellcheck = en_GB
%
\documentclass[presentation]{beamer}
\mode<presentation>{\usetheme{AMSCesenaPurpleAndGold}}
\setbeamertemplate{bibliography item}{\insertbiblabel}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage[english]{babel}
\usepackage[utf8]{inputenc}
%
\usepackage{SCO-EXTRAAMAS-2021-talk}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title[\gridex]{
    % same title of the presented paper
	\gridex: An Algorithm for
	\\
	Knowledge Extraction from Black-Box Regressors
}
%
% \subtitle{Extended Abstract}
%
% same authors order of the presented paper
\author[F. Sabbatini et al.]{
	\emph{Federico Sabbatini} % empth the presenting author
	\and 
	Giovanni Ciatto
	\\
	Andrea Omicini
}
%
\institute[UniBo]{
    Dipartimento di Informatica -- Scienza e Ingegneria (DISI)
    \\
    \textsc{Alma Mater Studiorum} -- Università di Bologna
    \\
    \texttt{
        \{\emph{f.sabbatini}, giovanni.ciatto, andrea.omicini\}@unibo.it % emph the presenting author's email
    }
}
%
\date[EXTRAAMAS, 2021]{
	3\textsuperscript{rd} International Workshop on EXplainable and
	\\
	TRAnsparent AI and Multi-Agent Systems (EXTRAAMAS)
	\\
	May 3, 2021, London, UK (online)
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\AtBeginSection[]
{
%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}<beamer>[c,noframenumbering]
\frametitle{Next in Line\ldots}
\tableofcontents[sectionstyle=show/shaded,subsectionstyle=hide]
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\
}
\AtBeginSubsection[]
{
%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}<beamer>[shrink,noframenumbering]
    \frametitle{Focus on\ldots}
	\mbox{~}
	\tableofcontents[currentsubsection,sectionstyle=shaded,subsectionstyle=show/shaded]
	\mbox{~}
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\
}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\begin{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

%\\\\\\\\\\\\\\\\\\\\\
\frame{\titlepage}
%\\\\\\\\\\\\\\\\\\\\\

%===============================================================================
\section{Context \& Motivation} 
%===============================================================================

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}[c]{Context}
    
    %
    \vfill
    %
    \begin{itemize}
        \item Focus on ML-based predictors:  many tasks, in different fields
        %
        \begin{itemize}
            \item[e.g.] pattern detection, image and speech recognition, clustering, classification and regression
        \end{itemize}
        
        \vfill
        
        \item Predictors are \emph{black boxes}: they hide their internal logic 
        %
        \begin{itemize}
            \item[!] only predictions are output to users: no \alert{explanation} provided
        \end{itemize}
        
        \vfill
        
        \item Lack of interpretability is not an option in critical applications
        %
        \begin{itemize}
        	\item[e.g.] healthcare, finance, etc.
        \end{itemize}
    
    	\vfill
        
        \item[$\rightarrow$] knowledge extraction mechanisms mitigate this issue
        
    \end{itemize}
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}[c]{Motivation}
	Several methods for knowledge extraction, however:
    %
    \vfill
    %
    \begin{itemize}
        \item most algorithms focus on classification tasks, neglecting regression
        %
        \begin{itemize}
            \item[e.g.] \textsc{Trepan}~\ccite{Craven1996}, Rule-extraction-as-learning~\ccite{craven1994using}, etc.
        \end{itemize}
        
        \vfill
        
        \item most methods supporting regression are over-constrained
        %
        \begin{itemize}
            \item[e.g.] \textsc{RefAnn}~\ccite{setiono2002extraction}, only applicable to ANNs with a single hidden layer
        \end{itemize}
        
        \vfill
        
        \item other methods performance degrades as complexity grows
        %
        \begin{itemize}
			\item[e.g.] \iter~\ccite{huysmans2006iter} with high-dimension data sets
		\end{itemize}
        
    \end{itemize}
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}[allowframebreaks]{Some state of the art}
	
    \begin{itemize}
    	\item ML-based predictors detect patterns and relationships buried in data
    	\begin{itemize}
    		\item the acquired knowledge is reused in similar applications
    	\end{itemize}
    
    	\bigskip
    
    	\item However, their knowledge is not explicitly represented
    	%
    	\begin{itemize}
    		\item[$\rightarrow$] lack of interpretability, models described as black boxes~\ccite{Lipton2016}
    		\item[$\rightarrow$] need of more interpretable predictors~\ccite{Rudin2019}, inspection techniques~\ccite{Guidotti2018}    		
    	\end{itemize}
    \end{itemize}

	\framebreak % se vuoi commenta fino a qui

	\begin{block}{Some definitions~\ccite{agentbasedxai-extraamas2020}}
		\begin{description}\small
			\item[interpretability] property of a system which is easily contemplable for humans  
			\item[explanation] a more interpretable model of a poorly-interpretable object
		\end{description}
	\end{block}

	\vspace{25px}
	\centering
	\includegraphics[width=.9\linewidth]{img/wb.png}	

	\framebreak

	\begin{block}{\iter~\ccite{huysmans2006iter} explains \textbf{regressors} via \texttt{if-then-else} rules}
		Pros:
		\begin{itemize}
			\item pedagogical method: no constraints on how regression works
			\item global approximation of the underlying black box 
		\end{itemize}

		Cons:
		\begin{itemize}
			\item possibly unable to cover the whole input space
			\item prediction discretisation: each rule predicts a constant
			\item inability to handle categorical data, only real vectors
		\end{itemize}
	\end{block}

%	\begin{alertblock}{Issues of \iter}
%		se vuoi sposta qui i cons di cui sopra
%	\end{alertblock}

%	\framebreak
%
%	\begin{block}{Why rules}
%		potresti spendere due parole sul perchè è interessante avere regole, in prospettiva
%	\end{block}

\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}{Contribution of the paper}

\begin{block}{The new \gridex algorithm extends \iter}
    \begin{itemize}
    	\item inheriting its core concepts
    	\item overcoming its major drawbacks
    	\begin{itemize}
    		\item[e.g.] non-exhaustivity and focus on low-importance input regions
    	\end{itemize}
        \item producing shorter rule lists
        \item retaining higher performance
    	\begin{itemize}
			\item predictions w.r.t. the data
			\item fidelity w.r.t. the underlying black box			
		\end{itemize}        
    \end{itemize}
\end{block}

\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

%===============================================================================
\section{\gridex design}
%===============================================================================

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}[allowframebreaks]
\frametitle{\iter basic idea}
	\begin{columns}[t]
		\column{0.51\linewidth}
			\centering
			\includegraphics[width = \columnwidth]{img/cubes.png}
		\column{0.48\linewidth}
			\centering
			\includegraphics[width = \columnwidth]{img/iter.png}
	\end{columns}
	\centering
	Hyper-cubes creation and expansion in the \iter algorithm, images from \ccite{huysmans2006iter}
	

	\framebreak
	
	\centering
	\includegraphics[width=.75\linewidth]{img/mlp.png}
	
	\begin{exampleblock}{Example: rule extraction from ANN via \iter}
		$ \text{if} ~ (v_1 \in [a_1, b_1], v_2 \in [a_2, b_2], v_3 \in [a_3, b_3], v_4 \in [a_4, b_4]) ~ \text{then} $ \\
		$ ~~~~~~~\text{output} = O $
	\end{exampleblock}	
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

\begin{frame}
\frametitle{\gridex design}
	\begin{columns}[t]
		\column{0.33\linewidth}
			\centering
			\includegraphics[width = \columnwidth]{img/demo/1.pdf}\\
			\scriptsize Input space and samples
		\column{0.33\linewidth}
			\centering
			\includegraphics[width = \columnwidth]{img/demo/2.pdf}\\
			\scriptsize First split, with $p_1=2$
		\column{0.33\linewidth}
			\centering
			\includegraphics[width = \columnwidth]{img/demo/3.pdf}\\
			\scriptsize Second split, with $p_2=3$
	\end{columns}
	\centering
	\vspace{10px} \gridex recursive input space splitting
\end{frame}

\begin{frame}
\frametitle{Differences between \iter and \gridex}
	\centering
	\begin{tabular}{l|l|l}
		& \iter & \gridex \\
		\hline\hline
		Approach & bottom-up & top-down \\
		Extracted rules & if-then-else & if-then-else \\
		Input region importance & no & yes \\
		Input feature importance & no & yes \\
		Input space coverage & non-exhaustive & exhaustive \\
		Suitable with few input features & yes & yes \\
		Suitable with many input features & no & yes \\
		Output value & constant & constant 
	\end{tabular}
\end{frame}

\section{Experiments \& Results}

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}%[allowframebreaks]
\frametitle{Case study}

    \begin{itemize}
    	\item \gridex performance tested w.r.t. \iter and decision tree regressors
    	\begin{itemize}
    		\item in order to have neutral benchmarks
    	\end{itemize}
    
    	\vfill
    	
    	\item Several data sets of growing dimensionality
    	 \begin{itemize}
    		\item from 2 to 11 input features
    	\end{itemize}
    
    	\vfill
    	
    	\item Results averaged upon 10 executions for each data set
    	
    	\vfill
    	
    	\item Metrics: number of extracted rules, MAE and R$^2$ value
    	\begin{itemize}
    		\item with respect to both the underlying predictor and the actual data
    	\end{itemize}
    	
    	\vfill
    	
    	\item Selection of the best algorithm parameters
    	\begin{itemize}
    		\item considering the trade-off between fidelity and readability (\# of rules)
    	\end{itemize}
    	
    	\vfill
    \end{itemize}

\end{frame}

\begin{frame}[allowframebreaks]
	\frametitle{Results}
	
	\begin{columns}
		\begin{column}{0.5\linewidth}
			Focus on:
			\begin{itemize}
				\item number of rules
				\item predictive performance w.r.t. the data
				\item fidelity w.r.t. the underlying predictor
				\item input space coverage
				\begin{itemize}
					\item \iter: 100\% only for simple data sets
					\item[$\rightarrow$] incomplete training sample coverage
					\item[$\rightarrow$] inability to predict every test sample
				\end{itemize}
			\end{itemize}
		\end{column}
		\begin{column}{0.5\linewidth}
			\centering
			\includegraphics[width=\columnwidth]{img/comp/rules.pdf}
			\\
			\scriptsize Number of extracted rules
		\end{column}		
	\end{columns}	

	\framebreak

	\frametitle{Results}
	\scriptsize
	\begin{columns}[t]
		\column{0.5\linewidth}
			\centering
			\includegraphics[width=.8\columnwidth]{img/comp/mae.pdf}\\
			MAE w.r.t. the data\vspace{10px}
			\includegraphics[width=.8\columnwidth]{img/comp/maefid.pdf}\\
			MAE w.r.t. the underlying black box
		\column{0.5\linewidth}
			\centering
			\includegraphics[width=.8\columnwidth]{img/comp/r2.pdf}\\
		 	R$^2$ value w.r.t. the data\vspace{10px}
			\includegraphics[width=.8\columnwidth]{img/comp/r2fid.pdf}\\
			R$^2$ value w.r.t. the underlying black box
	\end{columns}

	%	\subfloat[Number of extracted rules.]{
		%\subfloat[MAE with respect to the data.]{
		%\subfloat[MAE fidelity with respect to the underlying black box.]{
%		\subfloat[R$^2$ value with respect to the data.]{
		%\subfloat[R$^2$ value with respect to the underlying black box.]{
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

\section{Conclusions}

%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}%[allowframebreaks]
\frametitle{Conclusions}

\begin{block}{Summing up}
	The new \gridex algorithm is able to:
    %
    \begin{itemize}
        \item overcome the \iter non-exhaustivity
        \item focus only on the interesting regions of the input space
        \item assign different weights to the input features
        \item produce shorter rule lists than \iter
        \item attain better predictive performances than \iter
        \item simplify the parameter tuning phase
    \end{itemize}
\end{block}

\end{frame}

\begin{frame}
	\frametitle{Future works}

\begin{exampleblock}{Future works}
	Our next research efforts will be focused on:
	\begin{itemize}
		\item overcoming the remaining limitations of \iter and other algorithms
		\begin{itemize}
			\item[e.g.] address the inability to handle categorical features
			\item[e.g.] substitute the constant output value of the produced decision rules
		\end{itemize}	
		\item the enhancement of \gridex
	    \begin{itemize}
			\item[e.g.] automatise the adaptive split threshold selection of \gridex
		\end{itemize}		
	\end{itemize}
\end{exampleblock}

\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

%===============================================================================
\section*{}
%===============================================================================
\frame{\titlepage}

%===============================================================================
\section*{\bibname}
%===============================================================================

\setbeamertemplate{page number in head/foot}{}
%\\\\\\\\\\\\\\\\\\\\\
\begin{frame}[t,allowframebreaks,noframenumbering]\frametitle{\refname}
% \begin{frame}[c]\frametitle{\refname}
	\footnotesize
%	\scriptsize
    \bibliographystyle{plain}
	\bibliography{SCO-EXTRAAMAS-2021-talk}
\end{frame}
%\\\\\\\\\\\\\\\\\\\\\

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\end{document}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
